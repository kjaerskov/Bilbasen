---
title: "A web scraper for bilbasen.dk"
author: "Frederik Holmelund Kjaerskov"
date: "28 February 2018"
output: 
  html_document: 
    theme: null
  html_notebook: 
    theme: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Scraping an used car site - part I

I wanted to practice web scraping skills and looked for suited sites to train on. As I am planning on buying a car in a near future I headed to [bilbasen.dk](http://www.bilbasen.dk) (FYI, a Danish site for used cars). I was happy to discover that the site is fairly well structured and thus my choice was further justified. To sum it up, this small tutorial is on my first web scraper to fetch data on old cars!

## The code part
Searching the web I discovered `rvest`. A package for R that brands itself as *easy web scraping with R*. It is compatible with `magrittr` and we can thus apply these nifty looking chains using the `%>%` operator. I strongly recommend to get a bit acquainted with this package as its forward pipe operator (the `%>%`) can tremendously tidy up your R code. And yes, it is part of the **tidyverse** which is a must have for aspiring data scientists. I will not go into details with the pipe operator (frankly a bunch of smart people know much more about it) but just casually drop this link [magrittr](http://magrittr.tidyverse.org/).

After installing the package we load it.

```{r message=FALSE, warning=FALSE}
library('rvest')
```

Next we need to inspect the URL we want to crawl. Ultimately we want to read the content of that page and fetch its data of interest but before we do it, pay close attention to how the link itself is structured. Note how the search query is embedded in the link and how the page is paginated by the "page=1" at the end. The latter allow us to crawl trough all the pages using a simple page number index. Most sites have an advanced search pane where searchable parameters can easily be deciphered from the URL query string once the search is submitted. 

From the below string`vw?` and `&yearfrom=2010&yearto=2018` readily tells us to search for VW cars produced from 2010 to 2018. We limit the search to cars where tax is paid `&includewithoutvehicleregistrationtax=false` and avoid leased cars `&includeleasing=false`. The content of the webpage is stored in the list `website` using the `read_html` function from `rvest`.

```{r}
URLtoCrawl <- "https://www.bilbasen.dk/brugt/bil/vw?fuel=0&yearfrom=2010&yearto=2018&pricefrom=0&priceto=10000000&mileagefrom=-1&mileageto=10000001&zipcode=0000&includeengroscvr=false&includesellforcustomer=true&includewithoutvehicleregistrationtax=false&includeleasing=false&page=1"
website <- read_html(URLtoCrawl)
```
For us to crawl through the all pages we need the page count returned from our query string. This is a good starting point to introduce the `html_nodes` function. It allows us to search and return only specific HMTL nodes in our website. I have had great use of the [SelectorGadget](http://selectorgadget.com/) browser extension to Google Chrome. It turns the selection of the right node into a simple matter of pointing and clicking. Simply click the element you wish to extract and the gadget returns the HTML node (or XPath). This node we then enter in the `html_nodes` function and store its content in a new variable in R. 

To get the number of hits from the HTML query string we type:
```{r}
#Extract number of hits
NumberOfHits <- website %>% html_nodes(".scrambledtab") %>% html_text(trim = TRUE)
NumberOfHits
```
From the above we see that the first element in `NumberOfHits` contains the total number of hits and we can omit the rest. 

```{r}
#Keep the total number of hits only
NumberOfHits <- NumberOfHits[1]
```

Next we are going to do some regular expression magic (I called it magic but rather it is a cumbersome method that can surely be improved, suggestions are welcome). It is biased and totally on my own account when I say. "There is no web scraping without regular expressions". Again and again I am faced with the need to clean text strings using regular expressions and can really not understate their effectiveness (and trickyness). To get the numbers only with the right decimal pointer we apply the following:

```{r}
#Keep the number only
NumberOfHits <- as.numeric(gsub(".+\\((\\d+).","\\1",gsub("\\.","",NumberOfHits)))
NumberOfHits
```
There we go, we now know the number of hits. To find the number of pages we need to know the number of hits returned per page. This is usually something you decide on through your browser and let the page store a cookie on your computer. Is varies from page to page. For bilbasen.dk we divide by the standard page size which is 32 thus:

```{r}
#Calculate number of pages
NumberOfPages <- ceiling(NumberOfHits/32)
```

Eureka! Now we know the number of pages to crawl through. The rest is simply to loop through each page, grow a vector containing the data of interest and finally construct a data frame to contain it all. 
This method requires we initialize the vectors we grow inside our loop. 

```{r}
#Initialize vectors
CarmakeDetail <- NULL
CarURL <- NULL
CarConsumption <- NULL
CarMileage <- NULL
CarYear <- NULL
CarRegion <- NULL
CarPrice <- NULL
```
and next we construct the loop that will take us through all the seach result pages at bilbasen.dk.

```{r}
#Loop through the total number of pages
for(i in 1:NumberOfPages) {
  
  # Set URL to equal increasing page number denoted by i
  URLtoCrawl <- gsub(paste("page=",i-1,sep=""),paste("page=",i,sep=""),URLtoCrawl)
  website <- read_html(URLtoCrawl)
  
  #Detailed description of car make
  CarmakeDetail <- c(CarmakeDetail, website %>% html_nodes(".darkLink") %>% html_text(trim = TRUE))
  CarURL <- c(CarURL, website %>% html_nodes(".darkLink") %>% html_attr("href"))
  
  #Car price
  CarPrice <- c(CarPrice, website %>% html_nodes(".listing-price")  %>% html_text(trim = TRUE))
  
  #Car region
  CarRegion <- c(CarRegion, website %>% html_nodes(".listing-region")  %>% html_text(trim = TRUE))
  
  #Car metrics
  CarListingData <- website %>% html_nodes(".listing-data")  %>% html_text(trim = TRUE)
  CarConsumption <- c(CarConsumption, CarListingData[1:4==2])
  CarMileage <- c(CarMileage, CarListingData[1:4==3])
  CarYear <- c(CarYear, CarListingData[1:4==4])
}
```

Finally we create a data frame based on the above data vectors. 

```{R warning=FALSE}
# Create dataframe
df = data.frame(CarmakeDetail,
                URL = CarURL,
                Year = CarYear,
                Model = factor(gsub("^\\w+{1}[[:blank:]](.+)[[:blank:]]\\d,.*$","\\1",CarmakeDetail)),
                MakeModel = gsub("^(.+)[[:blank:]]\\d,.*$","\\1",CarmakeDetail),
                Region = CarRegion, 
                Make = factor(gsub("^((\\w+){1}\\w+).*$","\\1",CarmakeDetail)),
                Engine = as.numeric(gsub(",",".",gsub("^.+[[:blank:]](\\d,\\d+).*$","\\1",CarmakeDetail))),
                Doors = gsub("^.+[[:blank:]](\\w{2,})$","\\1",CarmakeDetail),
                Horsepower = as.numeric(gsub("^.+[[:blank:]](\\d{2,}).*$","\\1",CarmakeDetail)),
                YearNumeric = as.numeric(CarYear),
                Price = as.numeric(gsub("\\.","",gsub(" kr.", "", CarPrice))),
                Mileage = as.numeric(gsub("\\.","",CarMileage)),
                Consumption = as.numeric(gsub(",",".",(gsub(".*?([0-9]+,[0-9]).*", "\\1", CarConsumption))))
                )
```

So that's it. What did we get? Let summary give you a glimpse. 

```{r}
summary(df)
```

We have now scraped a website using the HTML query string and looping through all the search result pages. Helped by `rvest` and regular expressions. In another post I will use this scraper to fetch data and analyze it. 

You can find this RMarkdown source file as well as a cleaner R script from Github.

* Link [RMarkdown file](https://github.com/kjaerskov/Bilbasen/blob/master/ScrapeBilbasen.Rmd)
* Link [R script](https://github.com/kjaerskov/Bilbasen/blob/master/CrawlBilbasen.R)
