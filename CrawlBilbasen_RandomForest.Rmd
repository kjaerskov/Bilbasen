---
title: "Scrape a used car website and select a bargain car using Random Forest"
author: "Frederik Holmelund Kjaerskov"
date: "6 March 2018"
output:
  html_document:
    theme: null
  html_notebook:
    theme: paper
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Scraping a used car site - part II

As mentioned in [this post](https://r-tify.blogspot.dk/2018/03/a-web-scraper-for-bilbasendk.html) I built a web scraper for a used car site to practice web scraping skills. This time we go ahead and apply this scraper, collect used car data and train and apply a Random Forest algorithm to predict used car prices. As a bonus, we will interpretate the results from the Random Forest to select a bargain car. 

Because the scraper itself is explained in [my previous post](https://r-tify.blogspot.dk/2018/03/a-web-scraper-for-bilbasendk.html) I will simply just source it in addition to loading the necessary libraries. It sources all VW's from 2010 to 2018 currently on sale at bilbasen.dk. 

```{r warning=FALSE, message=FALSE}
library('ggplot2') # visualization
library('scales') # visualization
library('dplyr') # data manipulation
library('randomForest') # classification algorithm

source("~/Bilbasen/CrawlBilbasen.R") # Source the web crawler script
str(df)

# Rows with NA
length(df[is.na(df), ])
```
See that there are a few entries that contains NAs. For this example, we will not spend time performing imputation hence we simply omit all NA entries. 

```{r}
# Remove rows with n.a. data
df <- na.omit(df)
```

## Build the model

Now we got the data we need. Next is to train our Random Forest algorithm for us to predict used car prices. We therefore divide our data into two sets. A training data set and a test data set. There are ways to do this clever but for matters of simplicity we denote two thirds of the data as training data and the remaining third as test data. To subset the data we first reorder the data randomly and select the dimensions we wish to model on. In this case we will train our model using model name, year, engine size, horsepower, number of doors, mileage, consumption and sales region. 

```{r}
# Randomize order for us to split data set into training and test sets
set.seed(666) # I like heavy metal
dfmodel <- df[sample(nrow(df), nrow(df)),
              c("URL", "Model", "Year", "Engine", "Horsepower", 
                "Doors", "Price", "Mileage", "Consumption", "Region")]

# Train using two thirds of the dataset, test using the last third
trainingData <- dfmodel[1:ceiling(nrow(df)*(2/3)), ]
testData <- dfmodel[1:floor(nrow(df)*(1/3)), ]
```

For the modelling part we use the Random Forest algorithm implemented by the `RandomForest` package to perform regression on price given the variables and training data just mentioned and write:

```{r}
# Build the model using the specified variables
rfModelPrice <- randomForest(Price ~ Year + Engine + Mileage + Consumption + 
                             Doors + Horsepower + Model + Region, 
                             data = trainingData)

# View model stats
rfModelPrice
```

And the model is built! See that it does a fairly good job at explaining the variance from the training data. 

## Predict used car prices

Now, we turn to the price prediction using the test data and create a new data frame for the results. We calculate two new variables. An error term as the difference between the actual price and the predicted price and a boolean we call `CheapCar`. The cheap car boolean we define as an error term less than 0. It tells us that our model predicts a higher price as the one that is actually listed. This can imply three things. 

* Either our model is inaccurate:__( 
* There is something about the pricing the model doesn't accounf for, e.g. damages:-| 
* Or we struck gold:-). 

While especially the second guess is most likely also the case, we interpretate our result as a cheap car. That is we found a car that is priced lower than expected given our training data and at least a car worth checking out. We will turn to this in the last section of this post.  

```{r}
# Predict using the test set
predictionResult <- data.frame(PredictedPrice = predict(rfModelPrice, testData), 
                               ActualPrice = testData$Price,
                               Model = testData$Model)

# Error term
predictionResult$Error <- predictionResult$ActualPrice - predictionResult$PredictedPrice

# The car is cheap if its actual price is lower than the predicted
predictionResult$CheapCar <- predictionResult$Error < 0
```

Plotting the actual prices along the predictions we get a view on how well we predict. The dashed line marks prices where actual and predicted prices are equal and the cheap car boolean is used to color "cheap" cars blue. 

```{r dpi=36, out.width="600px", out.height="450px"}
# Plot actual vs. predicted price
ggplot(predictionResult, aes(x=ActualPrice, y=PredictedPrice, color = CheapCar)) +
        geom_point(alpha = 0.2) + 
        geom_abline(slope = 1, linetype="dashed") + 
        geom_text(aes(label = Model), check_overlap = TRUE, vjust = 1) + 
        theme(aspect.ratio = 1) + 
        coord_fixed(ratio=1) +
        theme_minimal()
```

See that certain cars are predicted fairly well and a few expensive cars a more off? The cars that our model does a good job at predicting are those small and medium sized cars that are well represented by numbers in the data set. We can visualize this by plotting the actual price versus the error term and observe the trend. 

```{r dpi=36, out.width="600px", out.height="450px"}
ggplot(predictionResult, aes(x=ActualPrice, y=Error)) +
geom_point(alpha=0.8) + 
geom_smooth(method = 'loess') +
geom_text(aes(label = Model), check_overlap = TRUE, vjust = 1) + 
xlab("Actual price") +
ylab("Error") +
theme_minimal()
```

To narrow it down further, we look at the errors for the specific VW models. Generally, the car models that account for most of the variance in the data set are most accurately fitted by the Random Forest algorithm and rare and often expensive cars are ill fitted and therefore harder to predict accurately.  

If we plot the VW models versus the error term using boxplots we get an idea of how the errors distribute for the given model. In the plot below I've added the average price as a label in blue as well as jittered data points to get a sense of the number of cars present. Actually, by consecutively plotting the blue label according to the number of times the specific model appears in the data set we get a rough indicator on the number of cars. The more blue the label, the more cars. 


```{r dpi=36, out.width="600px", out.height="450px"}
# Calculate average price pr. car model
avgPrice <- tapply(predictionResult$ActualPrice, predictionResult$Model, mean)

# Boxplots of errors vs. model 
ggplot(predictionResult, aes(x=reorder(Model, Error, fun=mean),y=Error)) +
  geom_boxplot(outlier.alpha = 0.5) + 
  geom_jitter(color="tomato", alpha = 0.3) + 
  geom_hline(yintercept = 0, linetype="dashed", color="blue") +
  xlab("Model") +
  ylab("Error") +
  coord_flip() +
  geom_text(aes(x = Model, y = min(Error), 
                label = paste("Avg. ",round(avgPrice[Model],0))),
            hjust=0.3, vjust=1.3, size = 3, color="blue", alpha = 0.5) +
  theme_minimal()
```

## Select a bargain car using Random Forest

So far we know that

* our model does a good job at predicting prices for models like VW Passat and Golf. These account for a large part of the data set and our model does its best to explain variances resulting from these.

* our boolean `CheapCar` can be interpretated as an indicator of cars that are priced lower than one (that is the model) would expect. 

So if I were bargain hunting a VW Passat, a car that right now would accomodate for most of my needs, I should at least check the one that achieves the minimum error. This last piece of code does this and the screenshot at the bottom concludes this post and displays *the bargain car* worth a visit at the time of writing. 

```{r}
# A possible cheap Passat to buy
idx <- which(predictionResult$Model == "Passat")
bestbuy <- idx[which.min(predictionResult$Error[idx])]

#URL of car
paste("http://www.bilbasen.dk", testData[bestbuy,"URL"], sep="")

#Open URL 
browseURL(paste("http://www.bilbasen.dk", testData[bestbuy,"URL"], sep=""),
                browser = "C:/Program Files (x86)/Google/Chrome/Application/chrome.exe")
```

* This code is available on Github. Please see [CrawlBilbasen_RandomForest.rmd](https://github.com/kjaerskov/Bilbasen/blob/master/CrawlBilbasen_RandomForest.Rmd)

